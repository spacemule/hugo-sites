<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on spacemule.net</title>
    <link>https://spacemule.net/posts/</link>
    <description>Recent content in Posts on spacemule.net</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Jan 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://spacemule.net/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Containerizing My Home Server</title>
      <link>https://spacemule.net/posts/podman/</link>
      <pubDate>Thu, 13 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://spacemule.net/posts/podman/</guid>
      <description>The need for change One of the best parts of maintaining a hobby server is setting it up. It&amp;rsquo;s fun to get everything working and figure out all of the quirks of the various pieces of software that have to work together on there. Once it&amp;rsquo;s working well, it&amp;rsquo;s a joy to let it sit, auto-update and handle its business for you. Conversely, the worst part of maintaining a hobby server is upgrading the underlying OS to a new release or migrating it to a new platform or machine.</description>
      <content>&lt;h2 id=&#34;the-need-for-change&#34;&gt;The need for change&lt;/h2&gt;
&lt;p&gt;One of the best parts of maintaining a hobby server is setting it up. It&amp;rsquo;s fun to get everything working and figure out all of the quirks of the various pieces of software that have to work together on there. Once it&amp;rsquo;s working well, it&amp;rsquo;s a joy to let it sit, auto-update and handle its business for you. Conversely, the worst part of maintaining a hobby server is upgrading the underlying OS to a new release or migrating it to a new platform or machine. I was without CalDAV and CardDAV for about six months because I was too busy to get around to migrating my old data to my new server when I migrated to a better machine.&lt;/p&gt;
&lt;p&gt;Choosing to containerize was a decision based on curiousity, hardware limitations, and a desire to learn something new. My initial set up in Israel was my wife&amp;rsquo;s old laptop. With an amazing 4 gigs of RAM, a 100Mbps network card, and a screaming fast dual core AMD E1-1500, it was not a machine that could handle virtualization. My current, new-to-me shiny quad-core i5-2310 with 8 gigs of RAM, 2 SSDs and 2 spinning disks each in a BTRFS raid1 configuration is Watson compared to that old laptop. It&amp;rsquo;s not modern. It also rarely shows a system load above 0.5, so why invest in more? Still, neither machine can handle the number of VMs I would like for some fun at home. I had a choice between containerization and bare metal, and I chose the tinkerer&amp;rsquo;s option.&lt;/p&gt;
&lt;p&gt;I am used to running a home server. I&amp;rsquo;ve done it on and off since around 2010. I started by just installing everything on there, locking it down, and never backing up. I learned that VMs are your friend after about a year. After many close calls without any serious data loss, I integrated duplicity with an offsite backup sometime around 2013. Things stayed like that for a while until I discovered LXD. Canonical&amp;rsquo;s wrapper around LXC makes containers that work like VMs. It&amp;rsquo;s lightweight and easy to use, but I found the containers to be less portable than VMs. You can use cloud-config to create a container there that just works, but its definition is still tied to the underlying hardware. LXD feels like a crutch to bring VM-style administration to the world of containerization. It works, it&amp;rsquo;s brilliant, but it&amp;rsquo;s stuck between two worlds. Try migrating an LXD image from one mserver to another. I&amp;rsquo;ve done it, but it&amp;rsquo;s not as simple as migrating a VM where all the hardware is abstracted away. I decided if I was going to do containerization, I&amp;rsquo;d do it right, so I looked into learning about Docker and ended up settling on using podman on my server.&lt;/p&gt;
&lt;h2 id=&#34;my-solution&#34;&gt;My solution&lt;/h2&gt;
&lt;p&gt;Since nothing on my server needs 24/7 availability, I prefer to apply patches and upgrades automatically and fix things that break when they break. MicroOS from openSUSE handles this for the base OS. It has never broken for me, and if it does, it has automatic rollbacks so I don&amp;rsquo;t usually have to deal with it. There&amp;rsquo;s a neat feature in podman that handles upgrades for containers: &lt;code&gt;podman auto-update&lt;/code&gt;. Run this command, and any container labelled with &lt;code&gt;io.containers.autoupdate=image&lt;/code&gt; will pull a new image and restart if there&amp;rsquo;s an update. There&amp;rsquo;s also a systemd timer that automates this and (at least on openSUSE) can be enabled with &lt;code&gt;systemctl [--user] enable podman-auto-update.timer&lt;/code&gt;. For anyone looking to switch from Docker, podman has the command &lt;code&gt;generate systemd. This generates a systemd service file that can be installed to run the containers as system services. If you want them running when the user is not logged in, you&#39;ll have to &lt;/code&gt;enable-linger` once on the account running the containers.&lt;/p&gt;
&lt;p&gt;Here are the steps:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. Once for each account that needs to run containers while not logged in run `loginctl enable-linger`. You may need to do this as root, then it&#39;s `loginctl enable-linger $NAME_OF_USER_TO_ENABLE`.
2. Enable auto-updates if desired---also once per account: `systemctl --user enable podman-auto-update.timer`.
3. Start the container or pod.
4. Create a systemd service file for the container or pod: `podman generate systemd --files --new --name $NAME_OF_CONTAINER_OR_POD`
5. Move the service files to `$HOME/.config/systemd/user/`
6. Reload the service files: `systemctl --user daemon-reload`
7. Enable the service: `systemctl --user enable [--now] $SERVICE_FILE_NAME`
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s a breakdown of the above for running nginx as a user named &amp;ldquo;kube&amp;rdquo;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kube@zoggamule:~&amp;gt; podman ps -a
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES
kube@zoggamule:~&amp;gt; loginctl enable-linger #You only ever need to do this once per account
kube@zoggamule:~&amp;gt; systemctl --user enable podman-auto-update.timer # Likewise, once per account here
Created symlink /home/kube/.config/systemd/user/timers.target.wants/podman-auto-update.timer → /usr/lib/systemd/user/podman-auto-update.timer.
kube@zoggamule:~&amp;gt; podman run -d --rm --label io.containers.autoupdate=image --name nginx -p 10080:80 registry.opensuse.org/opensuse/nginx
Trying to pull registry.opensuse.org/opensuse/nginx:latest...
Getting image source signatures
Copying blob 0507b53fa0e9 skipped: already exists
Copying blob 2c6dcd4bd9f9 done
Copying config 9ddf22c9a7 done
Writing manifest to image destination
Storing signatures
f1f2cb7f91f449695ea6bead6bedbb1c90e3cc40b831e044267367685f012bed
kube@zoggamule:~&amp;gt; podman ps -a
CONTAINER ID  IMAGE                                        COMMAND          CREATED         STATUS             PORTS                  NAMES
f1f2cb7f91f4  registry.opensuse.org/opensuse/nginx:latest  /usr/sbin/nginx  46 minutes ago  Up 46 minutes ago  0.0.0.0:10080-&amp;gt;80/tcp  nginx
kube@zoggamule:~&amp;gt; podman generate systemd --new --files --name nginx
/home/kube/container-nginx.service
kube@zoggamule:~&amp;gt; ls
bin  container-nginx.service
kube@zoggamule:~&amp;gt; mkdir -p .config/systemd/user
kube@zoggamule:~&amp;gt; mv container-nginx.service ~/.config/systemd/user/
kube@zoggamule:~&amp;gt; systemctl --user daemon-reload
kube@zoggamule:~&amp;gt; systemctl --user enable --now container-nginx.service
Created symlink /home/kube/.config/systemd/user/default.target.wants/container-nginx.service → /home/kube/.config/systemd/user/container-nginx.service.
kube@zoggamule:~&amp;gt; podman ps -a
CONTAINER ID  IMAGE                                        COMMAND          CREATED        STATUS            PORTS                  NAMES
1226a35e009d  registry.opensuse.org/opensuse/nginx:latest  /usr/sbin/nginx  7 seconds ago  Up 7 seconds ago  0.0.0.0:10080-&amp;gt;80/tcp  nginx
kube@zoggamule:~&amp;gt; systemctl --user restart container-nginx.service
kube@zoggamule:~&amp;gt; podman ps -a
CONTAINER ID  IMAGE                                        COMMAND          CREATED        STATUS            PORTS                  NAMES
4c47fb5ef7c0  registry.opensuse.org/opensuse/nginx:latest  /usr/sbin/nginx  2 seconds ago  Up 2 seconds ago  0.0.0.0:10080-&amp;gt;80/tcp  nginx
kube@zoggamule:~&amp;gt; curl localhost:10080
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the container with the nginx web server is
successfully installed and working.&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
kube@zoggamule:~&amp;gt; systemctl --user disable --now container-nginx.service
Removed /home/kube/.config/systemd/user/default.target.wants/container-nginx.service.
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;the-changes&#34;&gt;The changes&lt;/h2&gt;
&lt;p&gt;The main change is organizational. Rather than having a mess of files in &lt;code&gt;/etc/&lt;/code&gt; and data in &lt;code&gt;/var/&lt;/code&gt;, I put all of my data and configuration for all of my containers in one directory with a directory named for each service. For example, my analytics service, umami has a directory with environment files and database storage for the two containers it uses. The one exception are files served by my webserver. Nextcloud gets a &lt;code&gt;nextcloud/&lt;/code&gt; folder that holds its database data and environment files, but to make my life easier, I put its html directory in another directory with all of my other sites&#39; web directories. This simplifies the mounts on my nginx container.&lt;/p&gt;
&lt;p&gt;The other big change is I try to make everything as simple as possible. I have one template for creating a server block for a new subdomain on nginx. I try to keep it simple so that I can have a new service up in minutes. Just pull the image, create the directories, run it, and copy the config file for the web server. The only part that requires any thought is generating the ssl certificats. I still need to read the docs at &lt;a href=&#34;https://acme.sh&#34;&gt;acme.sh&lt;/a&gt; every time I do that. The advantage to keeping things simple is that even if something won&amp;rsquo;t migrate easily for whatever reason, it won&amp;rsquo;t be hard to recreate and I&amp;rsquo;ll never go another 6 months without a calendar.&lt;/p&gt;
&lt;p&gt;The last &lt;em&gt;major&lt;/em&gt; change I had to make to my workflow was dealing with config files. I always hated tweaking config files to get things working on a fresh install. Most containers I&amp;rsquo;ve worked with have their configs set to sane defaults and allow overrides via environment variables. This simplifies setup considerably. Instead of a config file, I now keep an environment file with &lt;em&gt;only&lt;/em&gt; my tweaks in it. It saves the trouble of searching through a default file for the right option, and really makes it clear which changes I made to the config if I&amp;rsquo;m too lazy to set up version control.&lt;/p&gt;
&lt;p&gt;I don&amp;rsquo;t use docker compose. Init systems work and do what they do well, so I stick with that. Supposedly, docker-compose works with podman now. I have not tried it because I don&amp;rsquo;t see the need. Anytime a service is offered as a docker-compose file, I simply make a new pod, put the containers into the pod manually, and then run &lt;code&gt;podman generate systemd&lt;/code&gt; on the pod. Sometimes the service files need to be edited to make sure things load in the right order. Simply modify the &lt;code&gt;Before=&lt;/code&gt;, &lt;code&gt;After=&lt;/code&gt;, &lt;code&gt;Wants=&lt;/code&gt;, or other relevant lines to your needs.&lt;/p&gt;
&lt;p&gt;Starting services within a container is refreshingly different than from on a VM. I have a container running SSH that&amp;rsquo;s used as a jump host. Instead of a calling a cryptic init script or relying on a systemd service file somewhere that&amp;rsquo;s doing who knows what, it simply runs &lt;code&gt;/usr/sbin/sshd -D -f /sshd/sshd_config -p 2222 -e&lt;/code&gt;. It&amp;rsquo;s simple to understand what it&amp;rsquo;s doing, and it&amp;rsquo;s easy to change to fit my needs exactly.&lt;/p&gt;
&lt;p&gt;Version control is so much easier. I am working on reorganizing my files and cleaning up some of the systemd service files so they don&amp;rsquo;t contain any sensitive information. Once that&amp;rsquo;s done, I&amp;rsquo;m putting my server&amp;rsquo;s setup on Github. Backing up data is important, but sometimes the hardest part of a restore is the infrastructure. With everything containerized, I just need a host that can run the containers, my data, and the scripts to start them. My server ran out of disk space because I accidentally had the container images stored on the wrong partition. I had to completely tear down all of my containers and reset podman. To my surprise, once I set the new location and started my systemd services, everything worked just fine. Although it was a bit slow to start as all of the images needed to be downloaded, everything started perfectly and picked up where it left off. It was as easy as migrating a VM. Had I used volume storage, this would not have been the case, but more on that later.&lt;/p&gt;
&lt;p&gt;Custom containers initially seem like a pain, but they&amp;rsquo;re worth it. I used to build mine on a local machine until I learned how to use the Open Build Service (OBS) at &lt;a href=&#34;https://build.opensuse.org&#34;&gt;build.opensuse.org&lt;/a&gt;. Instead of fighting someone else&amp;rsquo;s work, I get exactly what I want. One of my next posts will be a tutorial using the OBS.&lt;/p&gt;
&lt;h2 id=&#34;errata&#34;&gt;Errata&lt;/h2&gt;
&lt;p&gt;Initially, I followed tutorials, how-tos, and readmes to the last detail. I used container volumes for storage because that&amp;rsquo;s what everyone else seemed to be doing. These are obnoxiously abstracted. They are easy to forget about after a container has outlived its useful life, and they add complexity to a lot of common tasks. Docker&amp;rsquo;s documentation states that volumes are easier to backup or migrate than bind-mounted host directories, and then provides no proof of this fact. In reality, bind-mounting a host directory to a container has perfectly good performance and allows for easier maintainenance. For example, my nginx configs are in a system directory that&amp;rsquo;s mounted on my nginx container. I can edit the files with vim on the host, or I can mount the directory on my laptop and edit the files in any text editor. I can also mount subdirectories of one container&amp;rsquo;s bind mounts to another container without needing to expose the entire mount. I would not have this flexibility with a volume. The same goes for backups. (I&amp;rsquo;ve moved from duplicity to restic, by the way.) I don&amp;rsquo;t have to create a container to go mount all my volumes and back them up. I simply place directories that need to be backed up in one main directory and back that up daily. I have not found a compelling reason to use volumes over bind mounts. The system reset I mentioned above was not the first time I did that. When I migrated from the laptop to the i5, Nextcloud was storing its data in volumes. It took me 6 months to get it back up and running because I didn&amp;rsquo;t want to deal with getting the data out of those volumes and onto the new host. Had I used bind-mounted directories, &lt;code&gt;rsync -avz&lt;/code&gt; would have easily copied all the necessary data and put my site back up in minutes instead of months.&lt;/p&gt;
&lt;p&gt;If you need to move your container storage to a new location, you&amp;rsquo;ll lose any volumes when you run &lt;code&gt;podman system reset&lt;/code&gt;. This is another reason that I avoid storing most things in volumes. I have two exceptions: Data that doesn&amp;rsquo;t need be backed up or moved in a migration such as a redis cache, and volumes automatically created by containers. Occasionally, you&amp;rsquo;ll find a container that overzealously creates volumes or has a volume with an incorrect mount point in its dockerfile. In these cases, I just assign a volume to the mount to keep it out of my hair. If you don&amp;rsquo;t assign a named volume, your system will become cluttered with new, randomly named volumes every time the container starts.&lt;/p&gt;
&lt;p&gt;Testing migrations is dead simple. Simply copy a container&amp;rsquo;s mounts to a new location and see if it can work by running it with the mounts changed. If it does, then it&amp;rsquo;s easily migrated to a new host. There are more elegant solutions to this, but they&amp;rsquo;re overkill for a home server.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>These Bars</title>
      <link>https://spacemule.net/posts/these-bars/</link>
      <pubDate>Fri, 07 Jan 2022 14:41:36 +0200</pubDate>
      
      <guid>https://spacemule.net/posts/these-bars/</guid>
      <description>It&amp;rsquo;s my belief that a country song is not good because it&amp;rsquo;s good. It&amp;rsquo;s good because it&amp;rsquo;s a good country song. There is a Platonic ideal for a country song, and like Plato, the definition alludes me. While Steve Goodman may be crowned as king of the &amp;ldquo;perfect country and western song,&amp;rdquo; I don&amp;rsquo;t think it&amp;rsquo;s right to bestow such an honored title in perpetuity. I also don&amp;rsquo;t think it&amp;rsquo;s right that See the Big Man Cry should even have to compete with a novelty song and He Stopped Loving Her Today.</description>
      <content>&lt;p&gt;It&amp;rsquo;s my belief that a country song is not good because it&amp;rsquo;s good. It&amp;rsquo;s good because it&amp;rsquo;s a good country song. There is a Platonic ideal for a country song, and like Plato, the definition alludes me. While Steve Goodman may be crowned as king of the &amp;ldquo;perfect country and western song,&amp;rdquo; I don&amp;rsquo;t think it&amp;rsquo;s right to bestow such an honored title in perpetuity. I also don&amp;rsquo;t think it&amp;rsquo;s right that &lt;em&gt;See the Big Man Cry&lt;/em&gt; should even have to compete with a novelty song and &lt;em&gt;He Stopped Loving Her Today&lt;/em&gt;. It&amp;rsquo;s just not fair to Charlie. Yet, I still needed something to push me to right this wrong.&lt;/p&gt;
&lt;p&gt;On January 3, 2021, I was involved in a bicycle accident that tore my ACL and damaged my meniscus. I was presribed alternately Tramadex and Percocet for the pain of the recovery and reconstructive surgery. In the haze of pain killers, I used my time off from work to write the lyrics to a country song that I knew had to be written.&lt;/p&gt;
&lt;p&gt;It seemed wrong&amp;mdash;given that bars are both a place where alcohol is served and a means of confinement&amp;mdash;that only songs like &lt;em&gt;Still Doing Time&lt;/em&gt; even touched on this obvious pun. I had to fill that void. This is my attempt:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;The lights come on, it&#39;s time to go and settle the tab.
For all my lusts I&#39;ve wound up here and given all I had.
With shaking limbs I get up and make my way to the chair.
I&#39;ve no strength left to face the truth: Death is in the air.

[Chorus]
I&#39;m stuck inside these bars,
Why do I act so proud?
I tell myself these lies
and wear pride as a shroud.
I&#39;m stuck inside these bars
I&#39;ll wish the world goodbye
I&#39;m stuck inside these bars, and
I know that&#39;s where I&#39;ll die

I tell myself that I can change and make it come out right,
But change won&#39;t come and I know I&#39;ve taken my own life.
I try each day to show the world the image of a man,
and every night, behind these bars, I&#39;m back where I began.

Five years ago we tied the knot. Who knew on just a thread?
Now she is gone and my home is this concrete bed. 
I stole her years with drunken lies and ended it the night
I told her she&#39;d be better dead, and she said: &amp;quot;You&#39;re right.&amp;quot;

[Chorus]

The time has come. The shaking&#39;s gone. Fire&#39;s in my throat.
I feel the cold on my hands and pray my soul to float
Far away from where I sit, sentenced to my death.
My resting place behind this bar with liquor on my breath.

I&#39;m stuck inside these bars,
Why do I act so proud?
I tell myself these lies
and wear pride as a shroud.
I&#39;m stuck inside these bars
I&#39;ll wish the world goodbye
I&#39;m stuck inside these bars, and
I know that here I&#39;ll die
&lt;/code&gt;&lt;/pre&gt;</content>
    </item>
    
    <item>
      <title>Simple repository caching on openSUSE</title>
      <link>https://spacemule.net/posts/repo-cache/</link>
      <pubDate>Tue, 04 Jan 2022 15:53:36 +0200</pubDate>
      
      <guid>https://spacemule.net/posts/repo-cache/</guid>
      <description>One of the things I missed the most from my Debian/Ubuntu days when I switched to openSUSE about a year ago was apt-cacher-ng with proxy autodiscovery. I had apt-cacher-ng running on one computer at home, and all of my containers, VMs, and laptops would use that proxy when they were on the home network. Even better, they&amp;rsquo;d seemlessly switch back to the default repos if the proxy was unavailable.
To be clear, what I want is to have my computers use apt-cacher-ng as a proxy when available to download packages.</description>
      <content>&lt;p&gt;One of the things I missed the most from my Debian/Ubuntu days when I switched to openSUSE about a year ago was apt-cacher-ng with proxy autodiscovery. I had apt-cacher-ng running on one computer at home, and all of my containers, VMs, and laptops would use that proxy when they were on the home network. Even better, they&amp;rsquo;d seemlessly switch back to the default repos if the proxy was unavailable.&lt;/p&gt;
&lt;p&gt;To be clear, what I want is to have my computers use apt-cacher-ng as a proxy when available to download packages. When it&amp;rsquo;s unavailable either due to downtime, a bug, or me being on a different network, I want zypper to default to the standard repos.&lt;/p&gt;
&lt;p&gt;I tried to find the proper way to do this on openSUSE. I asked on reddit and telegram, but I found no reasonable solutions. I tried creating a transparent caching proxy with squid and some iptables rules, but due to zypper downloading from multiple repositories simultaneously, it needed a lot of hacks to cache the rpms properly. I wasn&amp;rsquo;t willing to risk a duct-taped together solution that could break any day. I tried writing a service for libzypp, but found the documentation unclear and lacking useful examples. Finally, I found a solution so simple I&amp;rsquo;m surprised I haven&amp;rsquo;t seen it published elsewhere.&lt;/p&gt;
&lt;p&gt;After playing with this for a few months on and off I wondered what the spec actually requires of a &lt;code&gt;.repo&lt;/code&gt; file, and I found &lt;a href=&#34;https://www.suse.com/support/kb/doc/?id=000019926&#34;&gt;this gem on SUSE&amp;rsquo;s site&lt;/a&gt;. You can specify more than one &lt;code&gt;baseurl&lt;/code&gt;, and their order sets their priority. I looked at the &lt;a href=&#34;https://www.mankier.com/8/zypper#Commands-Supported_URI_formats&#34;&gt;manpage for zypper&lt;/a&gt; and found that a proxy can be defined in the URI.&lt;/p&gt;
&lt;p&gt;The solution is as simple as editing the repo file and adding a &lt;code&gt;baseurl&lt;/code&gt; pointing to apt-cacher-ng  before the default &lt;code&gt;baseurl&lt;/code&gt;. For reference, here&amp;rsquo;s one of my &lt;code&gt;.repo&lt;/code&gt; files:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[repo-oss]
name=openSUSE-Tumbleweed-Oss
enabled=1
autorefresh=1
path=/
baseurl=http://download.opensuse.org/tumbleweed/repo/oss/?proxy=10.42.0.1:3128
baseurl=http://download.opensuse.org/tumbleweed/repo/oss/
keeppackages=0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I have noticed the occasional 404 error in the proxy that shouldn&amp;rsquo;t be happening. The errors only seem to occur with metadata files and not the actual packages, so it&amp;rsquo;s serving its main purpose and saving me plenty of time and our cell connection to the internet plenty of bandwidth.&lt;/p&gt;
&lt;p&gt;This may work for Fedora, RHEL, and the other rpm distros, but I don&amp;rsquo;t have the time or need to test it out right now.&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
